experiment:
    name: xtx_precision_perf
    seed: 1234
    repeats: 3
    warmup_iters: 1

matrix:
    M: 1500000
    N: 10000
    dtype_storage: fp32
    layout: row_major

chunking: 
    rows_per_chunk: 300000
    pinned_buffers: 1
    pinned_buffer_gb: 12

host_memory:
  numa_mode: manual          # manual / auto (auto = detect nodes, default spread)
  placement:
    - { node: 0, frac: 0.5 }
    - { node: 1, frac: 0.5 }
  threads_per_node: 8
  max_threads: 0
  pin_threads: true
  numa_aware: true

devices:
    - device_id: 1
      backend: cublas
      usestreams: true
      streams: 2


modes:
    - name: fp32
      input_dtype: fp32
      compute: fp32
      accumulate: fp32
      cublas_math_mode: DEFAULT
      algorithm: syrk # / gemm

    - name: tf32
      input_dtype: fp32
      compute: tf32
      accumulate: fp32
      cublas_math_mode: TF32
      algorithm: gemmEx
    
    - name: bf16
      input_dtype: bf16
      compute: tensorcore
      accumulate: fp32
      cublas_math_mode: TENSOR_OP
      cast_on_gpu: true
      algorithm: gemmEx 

    - name: fp16
      input_dtype: fp16
      compute: tensorcore
      accumulate: fp32
      cublas_math_mode: TENSOR_OP
      cast_on_gpu: true
      algorithm: gemmEx 

# gemm / syrk
xtx:
    triangle: lower
    alpha: 1.0
    beta_first: 0.0
    beta_rest: 1.0

metrics:
    time_gpu_only: true
    include_h2d: true
    compute_tflops: true
    reference_mode: fp32
    error_norm: frobenius

output: 
    result_csv: result.csv
    log_level: info

