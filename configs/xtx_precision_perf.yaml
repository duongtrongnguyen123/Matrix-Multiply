experiment:
    name: xtx_precision_perf
    seed: 1234
    repeats: 2
    warmup_iters: 0

matrix:
    M: 300000
    N: 10000
    dtype_storage: fp32
    layout: row_major

chunking: 
    rows_per_chunk: 10000
    pinned_buffers: 1

host_memory:
  numa_mode: manual          # manual / auto (auto = detect nodes, default spread)
  placement:
    - { node: 0, frac: 1}
    #- { node: 1, frac: 0.5}
  threads_per_node: 32
  max_threads: 0
  pin_threads: true
  numa_aware: true

devices:
    - device_id: 0
      backend: cublas
      usestreams: true
      streams: 2
    #- device_id: 1
    #  backend: cublas
    #  usestreams: true
    #  streams: 2


modes:
    - name: fp32
      input_dtype: fp32
      compute: fp32
      accumulate: fp32
      cublas_math_mode: DEFAULT
      algorithm: syrk 

    - name: fp32
      input_dtype: fp32
      compute: fp32
      accumulate: fp32
      cublas_math_mode: DEFAULT
      algorithm: gemm

    - name: tf32
      input_dtype: fp32
      compute: tf32
      accumulate: fp32
      cublas_math_mode: TF32
      algorithm: gemmEx
    
    - name: bf16
      input_dtype: bf16
      compute: tensorcore
      accumulate: fp32
      cublas_math_mode: TENSOR_OP
      cast_on_gpu: true
      algorithm: gemmEx 

    - name: fp16
      input_dtype: fp16
      compute: tensorcore
      accumulate: fp32
      cublas_math_mode: TENSOR_OP
      cast_on_gpu: true
      algorithm: gemmEx 

# gemm / syrk
xtx:
    triangle: lower
    alpha: 1.0
    beta_first: 0.0
    beta_rest: 1.0

metrics:
    time_gpu_only: true
    include_h2d: true
    compute_tflops: true
    reference_mode: fp32
    error_norm: frobenius

output: 
    result_csv: result.csv
    log_level: info

